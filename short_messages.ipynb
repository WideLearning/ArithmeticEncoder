{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install peft accelerate loralib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import TensorDataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    integrations,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Using custom data configuration default-c026a0da68931db6\n",
      "Found cached dataset text (/home/mike/.cache/huggingface/datasets/text/default-c026a0da68931db6/0.0.0)\n",
      "Loading cached processed dataset at /home/mike/.cache/huggingface/datasets/text/default-c026a0da68931db6/0.0.0/cache-4c558e740f1d48fa.arrow\n"
     ]
    }
   ],
   "source": [
    "base_name = \"ai-forever/rugpt3small_based_on_gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(base_name)\n",
    "\n",
    "full_dataset = Dataset.from_text(\"spam_messages_names.txt\")\n",
    "full_dataset = full_dataset.map(\n",
    "    lambda example: tokenizer(example[\"text\"]), batched=True\n",
    ")\n",
    "data_dict = full_dataset.train_test_split(test_size=0.01, shuffle=True, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61057 617\n",
      "{'text': 'Sōl Astrī: Не похоже', 'input_ids': [55, 134, 240, 80, 49062, 86, 133, 109, 30, 832, 5175], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "430\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(len(data_dict[\"train\"]), len(data_dict[\"test\"]))\n",
    "print(full_dataset[-1])\n",
    "lengths = np.array(sorted([len(elem[\"input_ids\"]) for elem in full_dataset]))\n",
    "print(lengths[-1])\n",
    "\n",
    "plt.plot(lengths)\n",
    "plt.ylim(0, 80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False  # freeze the model - train adapters later\n",
    "    if param.ndim == 1:\n",
    "        # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "        param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "\n",
    "\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x):\n",
    "        return super().forward(x).to(torch.float32)\n",
    "\n",
    "\n",
    "model.lm_head = CastOutputToFloat(model.lm_head)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mike/.local/lib/python3.10/site-packages/peft/tuners/lora.py:240: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 589824 || all params: 125821440 || trainable: 0.47%\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable: {100 * trainable_params / all_param:.2f}%\"\n",
    "    )\n",
    "\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,  # attention heads\n",
    "    lora_alpha=32,  # alpha scaling\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",  # set this for CLM or Seq2Seq\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=100,\n",
    "    max_steps=2,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=1,\n",
    "    output_dir=\"outputs\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    train_dataset=data_dict[\"train\"],\n",
    "    eval_dataset=data_dict[\"test\"],\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `GPT2LMHeadModel.forward` and have been ignored: text. If text are not expected by `GPT2LMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 617\n",
      "  Batch size = 8\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b65c9507c94b9a8386d6272d221a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_before = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.105103492736816, 'eval_runtime': 31.114, 'eval_samples_per_second': 19.83, 'eval_steps_per_second': 2.507}\n",
      "Results for a \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a }\\draw[            '#4',            line width=1\\pic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a  (Total): })    public static ObjectsByName()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a  is a cascade, a tin tail, an inverted taillet,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a ===&amp;gt; {        if(data &amp;gt;=\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a ql_replaceChar;    try    if ((isSimple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a icon: ico: ico: ico: ico ico: ico \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a u)}    # $(document).ready(function () {       \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">7</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Results for\"</span>, prompt)                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> it <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">range</span>(<span style=\"color: #0000ff; text-decoration-color: #0000ff\">10</span>):                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6 │   │   </span>tokens = tokenizer(prompt, return_tensors=<span style=\"color: #808000; text-decoration-color: #808000\">\"pt\"</span>)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 7 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>g = model.generate(**tokens, max_new_tokens=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">20</span>, return_dict_in_generate=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>, do    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 │   │   </span>prediction = g.sequences[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(tokenizer.decode(prediction))                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">10 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/mike/.local/lib/python3.10/site-packages/torch/utils/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">_contextlib.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">115</span> in                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">@functools</span>.wraps(func)                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">decorate_context</span>(*args, **kwargs):                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">114 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> ctx_factory():                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>115 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> func(*args, **kwargs)                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> decorate_context                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">118 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/mike/.local/lib/python3.10/site-packages/transformers/generation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1571</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">generate</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1568 │   │   │   </span>)                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1569 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1570 │   │   │   # 12. run sample</span>                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1571 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.sample(                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1572 │   │   │   │   </span>input_ids,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1573 │   │   │   │   </span>logits_processor=logits_processor,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1574 │   │   │   │   </span>logits_warper=logits_warper,                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/mike/.local/lib/python3.10/site-packages/transformers/generation/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2534</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">sample</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2531 │   │   │   </span>model_inputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.prepare_inputs_for_generation(input_ids, **model_kwargs)  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2532 │   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2533 │   │   │   # forward pass to get next token</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2534 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>(                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2535 │   │   │   │   </span>**model_inputs,                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2536 │   │   │   │   </span>return_dict=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>,                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2537 │   │   │   │   </span>output_attentions=output_attentions,                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/mike/.local/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/mike/.local/lib/python3.10/site-packages/transformers/models/gpt2/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_gpt2.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1046</span> in <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1043 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1044 │   │   </span>return_dict = return_dict <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_return  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1045 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1046 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>transformer_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.transformer(                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1047 │   │   │   </span>input_ids,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1048 │   │   │   </span>past_key_values=past_key_values,                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1049 │   │   │   </span>attention_mask=attention_mask,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/mike/.local/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/mike/.local/lib/python3.10/site-packages/transformers/models/gpt2/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_gpt2.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">889</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 886 │   │   │   │   │   </span>encoder_attention_mask,                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 887 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 888 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 889 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>outputs = block(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 890 │   │   │   │   │   </span>hidden_states,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 891 │   │   │   │   │   </span>layer_past=layer_past,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 892 │   │   │   │   │   </span>attention_mask=attention_mask,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/mike/.local/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/mike/.local/lib/python3.10/site-packages/transformers/models/gpt2/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_gpt2.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">426</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 423 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 424 │   │   </span>residual = hidden_states                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 425 │   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.ln_2(hidden_states)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 426 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>feed_forward_hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.mlp(hidden_states)                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 427 │   │   # residual connection</span>                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 428 │   │   </span>hidden_states = residual + feed_forward_hidden_states                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 429 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/mike/.local/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/mike/.local/lib/python3.10/site-packages/transformers/models/gpt2/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_gpt2.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">353</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 350 │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dropout = nn.Dropout(config.resid_pdrop)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 351 │   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 352 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, hidden_states: Optional[Tuple[torch.FloatTensor]]) -&gt; torch.FloatT  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 353 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.c_fc(hidden_states)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 354 │   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.act(hidden_states)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 355 │   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.c_proj(hidden_states)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 356 │   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dropout(hidden_states)                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/mike/.local/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/home/mike/.local/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">pytorch_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">112</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">109 │   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">110 │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, x):                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 │   │   </span>size_out = x.size()[:-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>] + (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.nf,)                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>112 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>x = torch.addmm(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias, x.view(-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, x.size(-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>)), <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.weight)                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 │   │   </span>x = x.view(size_out)                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">114 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> x                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m7\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mResults for\u001b[0m\u001b[33m\"\u001b[0m, prompt)                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mfor\u001b[0m it \u001b[95min\u001b[0m \u001b[96mrange\u001b[0m(\u001b[94m10\u001b[0m):                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 6 \u001b[0m\u001b[2m│   │   \u001b[0mtokens = tokenizer(prompt, return_tensors=\u001b[33m\"\u001b[0m\u001b[33mpt\u001b[0m\u001b[33m\"\u001b[0m)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 7 \u001b[2m│   │   \u001b[0mg = model.generate(**tokens, max_new_tokens=\u001b[94m20\u001b[0m, return_dict_in_generate=\u001b[94mTrue\u001b[0m, do    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m│   │   \u001b[0mprediction = g.sequences[\u001b[94m0\u001b[0m]                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(tokenizer.decode(prediction))                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m10 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/mike/.local/lib/python3.10/site-packages/torch/utils/\u001b[0m\u001b[1;33m_contextlib.py\u001b[0m:\u001b[94m115\u001b[0m in                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mdecorate_context\u001b[0m                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   \u001b[0m\u001b[1;95m@functools\u001b[0m.wraps(func)                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mdecorate_context\u001b[0m(*args, **kwargs):                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m114 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m ctx_factory():                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m115 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m func(*args, **kwargs)                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m decorate_context                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m118 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/mike/.local/lib/python3.10/site-packages/transformers/generation/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m1571\u001b[0m in \u001b[92mgenerate\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1568 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1569 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1570 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# 12. run sample\u001b[0m                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1571 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.sample(                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1572 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minput_ids,                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1573 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlogits_processor=logits_processor,                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1574 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mlogits_warper=logits_warper,                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/mike/.local/lib/python3.10/site-packages/transformers/generation/\u001b[0m\u001b[1;33mutils.py\u001b[0m:\u001b[94m2534\u001b[0m in \u001b[92msample\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2531 \u001b[0m\u001b[2m│   │   │   \u001b[0mmodel_inputs = \u001b[96mself\u001b[0m.prepare_inputs_for_generation(input_ids, **model_kwargs)  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2532 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2533 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# forward pass to get next token\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2534 \u001b[2m│   │   │   \u001b[0moutputs = \u001b[96mself\u001b[0m(                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2535 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m**model_inputs,                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2536 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mreturn_dict=\u001b[94mTrue\u001b[0m,                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2537 \u001b[0m\u001b[2m│   │   │   │   \u001b[0moutput_attentions=output_attentions,                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/mike/.local/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/mike/.local/lib/python3.10/site-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mmodeling_gpt2.py\u001b[0m:\u001b[94m1046\u001b[0m in \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mforward\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1043 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1044 \u001b[0m\u001b[2m│   │   \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_return  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1045 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1046 \u001b[2m│   │   \u001b[0mtransformer_outputs = \u001b[96mself\u001b[0m.transformer(                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1047 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_ids,                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1048 \u001b[0m\u001b[2m│   │   │   \u001b[0mpast_key_values=past_key_values,                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1049 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/mike/.local/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/mike/.local/lib/python3.10/site-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mmodeling_gpt2.py\u001b[0m:\u001b[94m889\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mforward\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 886 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mencoder_attention_mask,                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 887 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 888 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 889 \u001b[2m│   │   │   │   \u001b[0moutputs = block(                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 890 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mhidden_states,                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 891 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mlayer_past=layer_past,                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 892 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mattention_mask=attention_mask,                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/mike/.local/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/mike/.local/lib/python3.10/site-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mmodeling_gpt2.py\u001b[0m:\u001b[94m426\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mforward\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 423 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 424 \u001b[0m\u001b[2m│   │   \u001b[0mresidual = hidden_states                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 425 \u001b[0m\u001b[2m│   │   \u001b[0mhidden_states = \u001b[96mself\u001b[0m.ln_2(hidden_states)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 426 \u001b[2m│   │   \u001b[0mfeed_forward_hidden_states = \u001b[96mself\u001b[0m.mlp(hidden_states)                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 427 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# residual connection\u001b[0m                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 428 \u001b[0m\u001b[2m│   │   \u001b[0mhidden_states = residual + feed_forward_hidden_states                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 429 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/mike/.local/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/mike/.local/lib/python3.10/site-packages/transformers/models/gpt2/\u001b[0m\u001b[1;33mmodeling_gpt2.py\u001b[0m:\u001b[94m353\u001b[0m in  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[92mforward\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 350 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.dropout = nn.Dropout(config.resid_pdrop)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 351 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 352 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, hidden_states: Optional[Tuple[torch.FloatTensor]]) -> torch.FloatT  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 353 \u001b[2m│   │   \u001b[0mhidden_states = \u001b[96mself\u001b[0m.c_fc(hidden_states)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 354 \u001b[0m\u001b[2m│   │   \u001b[0mhidden_states = \u001b[96mself\u001b[0m.act(hidden_states)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 355 \u001b[0m\u001b[2m│   │   \u001b[0mhidden_states = \u001b[96mself\u001b[0m.c_proj(hidden_states)                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 356 \u001b[0m\u001b[2m│   │   \u001b[0mhidden_states = \u001b[96mself\u001b[0m.dropout(hidden_states)                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/mike/.local/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/home/mike/.local/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mpytorch_utils.py\u001b[0m:\u001b[94m112\u001b[0m in \u001b[92mforward\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m109 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m110 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, x):                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   \u001b[0msize_out = x.size()[:-\u001b[94m1\u001b[0m] + (\u001b[96mself\u001b[0m.nf,)                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m112 \u001b[2m│   │   \u001b[0mx = torch.addmm(\u001b[96mself\u001b[0m.bias, x.view(-\u001b[94m1\u001b[0m, x.size(-\u001b[94m1\u001b[0m)), \u001b[96mself\u001b[0m.weight)                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   │   \u001b[0mx = x.view(size_out)                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m114 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m x                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mKeyboardInterrupt\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(results_before)\n",
    "print(tokenizer.pad_token)\n",
    "tokenizer._pad_token = tokenizer._eos_token\n",
    "print(tokenizer.pad_token)\n",
    "for prompt in [\"a \", \"Mikhail Budnikov:\", \"Rodion Khvorostov:\", \"Veronika Sirotkina:\"]:\n",
    "    print(\"Results for\", prompt)\n",
    "    for it in range(10):\n",
    "        tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        g = model.generate(**tokens, max_new_tokens=20, return_dict_in_generate=True, do_sample=True)\n",
    "        prediction = g.sequences[0]\n",
    "        print(tokenizer.decode(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
