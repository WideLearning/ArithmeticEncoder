{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from fractions import Fraction\n",
    "from collections.abc import Generator\n",
    "from itertools import islice\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.lm = AutoModelForCausalLM.from_pretrained(\n",
    "            \"ai-forever/rugpt3small_based_on_gpt2\"\n",
    "        )\n",
    "\n",
    "    def vocab_size(self):\n",
    "        return 50264\n",
    "\n",
    "    def pmf(self, prefix: list[int]) -> list[float]:\n",
    "        # prefix = []\n",
    "        logits = self.lm(input_ids=torch.tensor(prefix + [0])).logits[-1]\n",
    "        assert logits.shape == (self.vocab_size(),)\n",
    "        return F.softmax(logits, dim=-1).tolist()\n",
    "\n",
    "    def cdf(self, prefix: list[int], denom=479001600) -> list[Fraction]:\n",
    "        # make EOF more likely\n",
    "        EOF = 50257\n",
    "        probs = self.pmf(prefix)\n",
    "        # print(\"\\t\\tEOF:\\t\", -log2(probs[EOF]))\n",
    "        probs[EOF] = max(min(probs[EOF] * 30e6, 0.5), 0.01)\n",
    "\n",
    "        probs = [0.0] + probs\n",
    "        for i in range(self.vocab_size()):\n",
    "            probs[i + 1] = probs[i] + max(probs[i + 1], 2 / denom)\n",
    "        for i in range(self.vocab_size()):\n",
    "            probs[i + 1] /= probs[-1]\n",
    "            probs[i + 1] = Fraction(int(probs[i + 1] * denom), denom)\n",
    "        for i in range(self.vocab_size()):\n",
    "            assert probs[i] < probs[i + 1]\n",
    "        assert probs[0] == 0 and probs[-1] == 1\n",
    "        return probs\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ai-forever/rugpt3small_based_on_gpt2\")\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_range(tokens: list[int], model: Model) -> tuple[Fraction, Fraction]:\n",
    "    \"\"\"Returns the range corresponding to the given sequence of tokens. Its length equal the probability of the sequence.\n",
    "\n",
    "    Args:\n",
    "        tokens (list[int]): Message to encode.\n",
    "        model (Model): Specifies distribution of possible sequences, by giving probabilities for each token conditional on the ones before.\n",
    "\n",
    "    Returns:\n",
    "        tuple[float, float]: (l, r)\n",
    "    \"\"\"\n",
    "    start = Fraction(0)\n",
    "    length = Fraction(1.0)\n",
    "    for i in range(len(tokens)):\n",
    "        cdf = model.cdf(tokens[:i])\n",
    "        c = tokens[i]\n",
    "        start += length * cdf[c]\n",
    "        length *= cdf[c + 1] - cdf[c]\n",
    "        print(f\"{i}: {-log2(float(length))}\")\n",
    "    return start, start + length\n",
    "\n",
    "\n",
    "def range_to_digits(lef: Fraction, rig: Fraction, model: Model, base: int) -> list[int]:\n",
    "    \"\"\"Return the shortest number in [lef, rig) with base `base`.\n",
    "\n",
    "    Args:\n",
    "        base (int): Base of encoding.\n",
    "\n",
    "    Returns:\n",
    "        list[int]: Digits of the result.\n",
    "    \"\"\"\n",
    "    digits = []\n",
    "    while not (lef <= 0 < rig):\n",
    "        lef *= base\n",
    "        rig *= base\n",
    "        d = int(rig)\n",
    "        digits.append(d)\n",
    "        lef -= d\n",
    "        rig -= d\n",
    "    return digits\n",
    "\n",
    "\n",
    "def digits_to_number(digits: list[int], base: int) -> Fraction:\n",
    "    number = Fraction(0)\n",
    "    for d in reversed(digits):\n",
    "        number = (number + d) / base\n",
    "    return number\n",
    "\n",
    "\n",
    "def number_to_tokens(\n",
    "    number: Fraction, model: Model, eof_token: int = None\n",
    ") -> Generator[int]:\n",
    "    prefix = []\n",
    "    while True:\n",
    "        cdf = model.cdf(prefix)\n",
    "        c = next(i for i in range(len(cdf)) if number < cdf[i + 1])\n",
    "        assert cdf[c] <= number < cdf[c + 1]\n",
    "        yield c\n",
    "        if c == eof_token:\n",
    "            break\n",
    "        prefix.append(c)\n",
    "        number = (number - cdf[c]) / (cdf[c + 1] - cdf[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_char_repr = \"{'a': [' ', 'n', 't', 'l', 'r', 's', 'c', 'd', 'i', 'm', 'b', 'y', 'v', 'g', 'p', 'u', 'k', 'f', 'w', 'x', 'h', 'e', 'z', 'j', 'o', 'a', 'q'], 'b': [' ', 'e', 'l', 'o', 'u', 'y', 'a', 'r', 'i', 's', 'j', 't', 'b', 'v', 'm', 'd', 'n', 'c', 'h', 'p'], 'c': [' ', 'o', 'e', 'h', 'a', 't', 'i', 'u', 'r', 'l', 'k', 'c', 'y', 's', 'q', 'm', 'd', 'f', 'p', 'g', 'n', 'z', 'b'], 'd': [' ', 'e', 'i', 'o', 'a', 'u', 's', 'r', 'y', 'd', 'l', 'g', 'v', 'm', 'w', 'n', 'h', 'j', 't', 'f', 'b', 'c', 'p', 'q'], 'e': [' ', 'r', 'n', 's', 'd', 'a', 'l', 'c', 't', 'e', 'm', 'v', 'x', 'i', 'p', 'f', 'y', 'g', 'w', 'o', 'q', 'u', 'b', 'h', 'k', 'j', 'z'], 'f': [' ', 'o', 'i', 'e', 'r', 'a', 'f', 'u', 't', 'l', 'y', 's', 'm', 'c'], 'g': [' ', 'e', 'h', 'r', 'i', 'a', 'o', 'u', 'n', 'l', 's', 'y', 'g', 't', 'm', 'd', 'f', 'w'], 'h': [' ', 'e', 'a', 'i', 'o', 't', 'r', 'u', 'y', 'n', 's', 'm', 'l', 'w', 'b', 'd', 'f', 'c', 'p', 'h'], 'i': [' ', 'n', 's', 't', 'o', 'c', 'l', 'e', 'm', 'r', 'd', 'v', 'a', 'g', 'f', 'b', 'p', 'z', 'k', 'i', 'x', 'u', 'q', 'h', 'j', 'w'], 'j': [' ', 'u', 'o', 'e', 'a', 'i'], 'k': [' ', 'e', 'i', 'n', 's', 'a', 'l', 'o', 'y', 'h', 'u', 'r', 'g', 'w', 'm', 'f', 't', 'b', 'p', 'd'], 'l': [' ', 'e', 'i', 'l', 'a', 'y', 'o', 'd', 's', 'u', 't', 'f', 'v', 'm', 'k', 'p', 'w', 'c', 'r', 'b', 'g', 'n', 'h'], 'm': [' ', 'e', 'a', 'o', 'i', 'p', 'u', 'm', 's', 'b', 'y', 'n', 'l', 'c', 'f', 'r', 't', 'g', 'd', 'w', 'h'], 'n': [' ', 'd', 't', 'g', 'e', 's', 'o', 'c', 'a', 'i', 'y', 'u', 'n', 'f', 'l', 'v', 'k', 'm', 'j', 'h', 'r', 'p', 'q', 'w', 'z', 'b', 'x'], 'o': [' ', 'n', 'r', 'f', 'u', 'm', 't', 'l', 'w', 's', 'p', 'o', 'd', 'v', 'c', 'b', 'g', 'i', 'k', 'a', 'e', 'y', 'h', 'x', 'j', 'z', 'q'], 'p': [' ', 'e', 'r', 'o', 'a', 'l', 'p', 'i', 't', 'u', 'h', 's', 'm', 'y', 'f', 'b', 'w', 'd', 'n', 'c', 'k'], 'q': [' ', 'u'], 'r': [' ', 'e', 'i', 'o', 'a', 's', 't', 'y', 'd', 'm', 'n', 'u', 'c', 'r', 'g', 'k', 'l', 'v', 'p', 'f', 'b', 'h', 'w', 'x', 'q', 'z', 'j'], 's': [' ', 't', 'e', 'i', 's', 'o', 'h', 'u', 'a', 'p', 'c', 'm', 'y', 'l', 'k', 'w', 'f', 'n', 'b', 'q', 'r', 'd', 'g', 'v'], 't': [' ', 'h', 'i', 'e', 'o', 'a', 'r', 's', 'u', 'y', 't', 'l', 'w', 'm', 'c', 'n', 'f', 'p', 'z', 'b', 'g', 'd', 'v'], 'u': [' ', 'r', 's', 't', 'n', 'l', 'c', 'e', 'm', 'a', 'p', 'g', 'i', 'd', 'b', 'f', 'o', 'k', 'y', 'x', 'v', 'z', 'h', 'u'], 'v': [' ', 'e', 'i', 'a', 'o', 'y', 'u', 'r', 's'], 'w': [' ', 'a', 'h', 'i', 'e', 'o', 'n', 's', 'r', 'l', 't', 'd', 'y', 'f', 'm', 'k', 'b', 'p', 'u', 'c'], 'x': [' ', 'p', 't', 'i', 'a', 'c', 'e', 'u', 'h', 'x', 'o', 'y', 'v', 'f', 'l'], 'y': [' ', 'o', 's', 'e', 'i', 'p', 'm', 't', 'a', 'l', 'c', 'n', 'r', 'd', 'b', 'w', 'g', 'z', 'u', 'f'], 'z': [' ', 'e', 'a', 'i', 'o', 'z', 'y', 'u', 'l', 'h'], ' ': 'abcdefghijklmnopqrstuvwxyz'}\"\n",
    "next_char = eval(next_char_repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21560:\t Wh\n",
      "83:\t Who\n",
      "10935:\t Who could\n",
      "3970:\t Who could have\n",
      "29071:\t Who could have thought\n",
      "35:\t Who could have thought?\n",
      "50257:\t Who could have thought?<|endoftext|>\n",
      "0: 27.25049273335425\n",
      "1: 39.261399367681115\n",
      "2: 54.18200235124152\n",
      "3: 66.43314096667855\n",
      "4: 78.34081823867162\n",
      "5: 83.58264288270904\n",
      "6: 86.37907667916825\n",
      "83 digits: 11101100110010110010100000011111101100011101100110011000110100110100111100110011011\n",
      " hyiloirzelpuiq blbde\n"
     ]
    }
   ],
   "source": [
    "base = 2\n",
    "\n",
    "text = \"Who could have thought?\"\n",
    "EOF = 50257\n",
    "tokens = tokenizer.encode(text) + [EOF]\n",
    "for i in range(len(tokens)):\n",
    "    print(f\"{tokens[i]}:\\t {tokenizer.decode(tokens[:i+1])}\")\n",
    "\n",
    "lef, rig = tokens_to_range(tokens, model)\n",
    "tot = rig - lef\n",
    "dig = range_to_digits(lef, rig, model, base=base)\n",
    "print(len(dig), \"digits:\", \"\".join(map(str, dig)))\n",
    "num = digits_to_number(dig, base=base)\n",
    "# print(num, float(num))\n",
    "assert tokens == list(islice(number_to_tokens(num, model, EOF), 20))\n",
    "\n",
    "integer = 0\n",
    "for d in reversed(dig):\n",
    "    integer = 2 * integer + d\n",
    "out = \" \"\n",
    "while integer:\n",
    "    subset = next_char[out[-1]]\n",
    "    out += subset[integer % len(subset)]\n",
    "    integer //= len(subset)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
